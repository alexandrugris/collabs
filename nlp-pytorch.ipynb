{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torchtext portalocker spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0           1\n",
      "0  Go.        Va !\n",
      "1  Go.     Marche.\n",
      "2  Go.  En route !\n",
      "3  Go.     Bouge !\n",
      "4  Hi.     Salut !\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./.data/datasets/fra.txt\", delimiter='\\t', header=None)[[0, 1]]\n",
    "print(df.head())\n",
    "df = np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m spacy download en_core_web_sm\n",
    "# !python3 -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17929 29072\n"
     ]
    }
   ],
   "source": [
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "fr_tokenizer = get_tokenizer('spacy', language='fr_core_news_sm')\n",
    "\n",
    "en_counter = Counter()\n",
    "fr_counter = Counter()\n",
    "\n",
    "_df = df\n",
    "df = []\n",
    "\n",
    "for e, f in _df:\n",
    "\n",
    "    et = en_tokenizer(e)\n",
    "    ft = fr_tokenizer(f)\n",
    "\n",
    "    en_counter.update(et)\n",
    "    fr_counter.update(ft)\n",
    "\n",
    "    df.append([et, ft])\n",
    "\n",
    "print(len(en_counter), len(fr_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab = torchtext.vocab.vocab(en_counter, specials=['<unk>', '<pad>', '<eos>'], special_first=True)\n",
    "fr_vocab = torchtext.vocab.vocab(fr_counter, specials=['<unk>', '<pad>', '<eos>', '<sos>'], special_first=True)\n",
    "\n",
    "PAD_EN = en_vocab['<pad>']\n",
    "PAD_FR = fr_vocab['<pad>']\n",
    "EOS_EN = en_vocab['<eos>']\n",
    "EOS_FR = fr_vocab['<eos>']\n",
    "SOS_FR = fr_vocab['<sos>']\n",
    "\n",
    "en_vocab.set_default_index = en_vocab['<unk>']\n",
    "fr_vocab.set_default_index = fr_vocab['<unk>']\n",
    "\n",
    "del en_counter\n",
    "del fr_counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(batch):\n",
    "    eb = []\n",
    "    fb = [] \n",
    "    for e, f in batch:\n",
    "        tte = torch.LongTensor([en_vocab[k] for k in e])\n",
    "        ttf = torch.LongTensor([fr_vocab[k] for k in f])\n",
    "\n",
    "        eb.append(torch.cat([tte, torch.LongTensor([EOS_EN])], dim=0))\n",
    "        fb.append(torch.cat([ttf, torch.LongTensor([EOS_FR])], dim=0))\n",
    "\n",
    "        en_batch = pad_sequence(eb, batch_first=True, padding_value=PAD_EN)\n",
    "        fr_batch = pad_sequence(fb, batch_first=True, padding_value=PAD_FR)\n",
    "\n",
    "    return en_batch, fr_batch \n",
    "\n",
    "dl = DataLoader(df, batch_size=128, shuffle=True, collate_fn=make_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 21]) torch.Size([128, 26])\n",
      "tensor([ 278,  322,   40,  811, 4424,    9,    2,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1])\n"
     ]
    }
   ],
   "source": [
    "_, batch = next(enumerate(dl))\n",
    "print(batch[0].shape, batch[1].shape)\n",
    "print(batch[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 21, 64])\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "\n",
    "        # just 1 lstm layer\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True, bidirectional=False)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.embedding(input)\n",
    "        output, _ = self.lstm(embedded)\n",
    "        return output\n",
    "\n",
    "# test\n",
    "enc = Encoder(len(en_vocab), 64)\n",
    "o_encoder = enc.forward(batch[0])\n",
    "\n",
    "print(o_encoder.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 27, 64]) torch.Size([128, 27])\n"
     ]
    }
   ],
   "source": [
    "class PreAttentionDecoder(nn.Module):\n",
    "    ### TODO: implement teacher forcing, add to each sequence a <SOS> character\n",
    "    \n",
    "    def __init__(self, target_vocab, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(target_vocab, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True, bidirectional=False)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        # shift right with the SOS token\n",
    "        shift_right = torch.LongTensor([SOS_FR] * input.shape[0]).reshape((input.shape[0], 1)).to(device)\n",
    "        input = torch.cat([shift_right, input], dim=1)\n",
    "\n",
    "        mask = torch.where(input != 0, 1, 0)\n",
    "        embedded = self.embedding(input)\n",
    "        output, _ = self.lstm(embedded)\n",
    "        return output, mask\n",
    "\n",
    "pad = PreAttentionDecoder(len(fr_vocab), 64).to(device)\n",
    "o_preattn, m_preattn = pad.forward(batch[1].to(device))\n",
    "\n",
    "print(o_preattn.shape, m_preattn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0800,  0.0390, -0.2563,  ...,  0.1186,  0.1022,  0.0967],\n",
      "        [ 0.0809,  0.0392, -0.2535,  ...,  0.1179,  0.1019,  0.0969],\n",
      "        [ 0.0791,  0.0383, -0.2566,  ...,  0.1200,  0.1026,  0.0973],\n",
      "        ...,\n",
      "        [ 0.0833,  0.0399, -0.2516,  ...,  0.1151,  0.1019,  0.0940],\n",
      "        [ 0.0833,  0.0399, -0.2516,  ...,  0.1151,  0.1019,  0.0940],\n",
      "        [ 0.0833,  0.0399, -0.2516,  ...,  0.1151,  0.1019,  0.0940]],\n",
      "       device='mps:0', grad_fn=<SelectBackward0>) torch.Size([27, 64])\n"
     ]
    }
   ],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, encoder_output, pre_attention_decoder_output, pre_attention_decoder_mask):\n",
    "        key = encoder_output # what is input\n",
    "        query = pre_attention_decoder_output # what has been so far translated\n",
    "        value = encoder_output # the value we want to translate\n",
    "\n",
    "        dk = torch.sqrt(torch.Tensor([key.shape[-1]])).to(device)\n",
    "        key_transpose = torch.transpose(key, 1, 2)\n",
    "\n",
    "        #print(dk, key.shape, query.shape, value.shape, key_transpose.shape)\n",
    "\n",
    "        mm = torch.bmm(query, key_transpose) / dk\n",
    "        mm = nn.functional.softmax(mm, dim=-1)\n",
    "\n",
    "        #print(mm.shape, mm[0])\n",
    "    \n",
    "        return torch.bmm(mm, value) * pre_attention_decoder_mask[:, :, None]\n",
    "    \n",
    "\n",
    "dpa = ScaledDotProductAttention().to(device)\n",
    "o_attn = dpa.forward(o_encoder.to(device), o_preattn.to(device), m_preattn.to(device))\n",
    "\n",
    "print(o_attn[0], o_attn[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 29076])\n",
      "tensor(18433, device='mps:0') tensor(-8.5180, device='mps:0', grad_fn=<MaxBackward1>) tensor([-10.2750, -10.2051, -10.5931, -10.6331, -10.4142, -10.4906, -10.1292,\n",
      "         -9.9625, -10.7294, -10.8555], device='mps:0',\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        NUM_LAYERS = 2\n",
    "\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=NUM_LAYERS)\n",
    "    \n",
    "        self.seq = nn.Sequential(\n",
    "            nn.BatchNorm1d(hidden_size * NUM_LAYERS),\n",
    "            nn.Linear(hidden_size * NUM_LAYERS, hidden_size * 2),\n",
    "            nn.BatchNorm1d(hidden_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size * 2, len(fr_vocab)), \n",
    "            nn.LogSoftmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward (self, input):\n",
    "\n",
    "        _, h = self.gru(input)\n",
    "\n",
    "        # batch first\n",
    "        h = torch.permute(h, (1, 0, 2))\n",
    "\n",
    "        # flatten\n",
    "        h = torch.reshape(h, (h.shape[0], h.shape[1] * h.shape[2]))\n",
    "\n",
    "        return self.seq(h)\n",
    "    \n",
    "dec = Decoder(64).to(device)\n",
    "o = dec.forward(o_attn)\n",
    "print(o.shape)\n",
    "print(torch.argmax(o[0]), torch.max(o[0]), o[0][0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training now, will get input and predict the last word. boom\n",
    "class Translator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(len(en_vocab), 64)\n",
    "        self.pre_attn_decoder = PreAttentionDecoder(len(fr_vocab), 64)\n",
    "        self.attention = ScaledDotProductAttention()\n",
    "        self.decoder = Decoder(64)\n",
    "\n",
    "    def forward(self, sentence, translated_sequence_so_far):\n",
    "\n",
    "        enc = self.encoder.forward(sentence)\n",
    "        pre_attn, mask = self.pre_attn_decoder.forward(translated_sequence_so_far)\n",
    "        attn = self.attention.forward(enc, pre_attn, mask)\n",
    "        return self.decoder.forward(attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translator(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(17932, 64)\n",
      "    (lstm): LSTM(64, 64, batch_first=True)\n",
      "  )\n",
      "  (pre_attn_decoder): PreAttentionDecoder(\n",
      "    (embedding): Embedding(29076, 64)\n",
      "    (lstm): LSTM(64, 64, batch_first=True)\n",
      "  )\n",
      "  (attention): ScaledDotProductAttention()\n",
      "  (decoder): Decoder(\n",
      "    (gru): GRU(64, 64, num_layers=2, batch_first=True)\n",
      "    (seq): Sequential(\n",
      "      (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=128, out_features=29076, bias=True)\n",
      "      (5): LogSoftmax(dim=-1)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Total params:  6892820\n",
      "loss: 10.340668  [    0/ 1703]\n"
     ]
    }
   ],
   "source": [
    "tn = Translator().to(device)\n",
    "print(tn)\n",
    "print(\"Total params: \", sum(p.numel() for p in tn.parameters() if p.requires_grad))\n",
    "\n",
    "# training\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(tn.parameters(), lr=1e-4) \n",
    "\n",
    "def train(model, dataloader):\n",
    "    size = len(dataloader)\n",
    "    model.train()\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        loss = 0\n",
    "\n",
    "        for i in range(0, y.shape[1]):\n",
    "            mask = torch.LongTensor([1] * i + [0] * (y.shape[1] - i)).to(device)\n",
    "            y_masked = y * mask[None, :]\n",
    "            result = y[:, i]\n",
    "\n",
    "            # Compute prediction error\n",
    "            pred = model(X, y_masked)\n",
    "            loss += loss_fn(pred, result)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), (batch)\n",
    "            loss /= y.shape[1] # normalize with the length of the batch\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "train(tn, dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(str_en):\n",
    "    cnt = 2 * len(str_en)\n",
    "    str_en = en_tokenizer(str_en)\n",
    "\n",
    "    # en\n",
    "    en = torch.LongTensor([en_vocab[k] for k in str_en] + [EOS_EN]).to(device)\n",
    "    # batch dimension\n",
    "    en = en[None, :]\n",
    "    print(en.shape)\n",
    "    ret = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = -1\n",
    "\n",
    "        while pred != EOS_FR and cnt > 0:\n",
    "            so_far = torch.LongTensor(ret)[None, :].to(device)\n",
    "            pred = torch.argmax(tn(en, so_far))\n",
    "            ret.append(pred.item())\n",
    "            cnt -= 1\n",
    "\n",
    "    print(fr_vocab.lookup_tokens(ret))\n",
    "\n",
    "\n",
    "translate(\"Let's go! Let's move from here!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
