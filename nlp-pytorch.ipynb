{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torchtext portalocker spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0           1\n",
      "0  Go.        Va !\n",
      "1  Go.     Marche.\n",
      "2  Go.  En route !\n",
      "3  Go.     Bouge !\n",
      "4  Hi.     Salut !\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./.data/datasets/fra.txt\", delimiter='\\t', header=None)[[0, 1]]\n",
    "print(df.head())\n",
    "df = np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m spacy download en_core_web_sm\n",
    "# !python3 -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15959 27135\n"
     ]
    }
   ],
   "source": [
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "fr_tokenizer = get_tokenizer('spacy', language='fr_core_news_sm')\n",
    "\n",
    "en_counter = Counter()\n",
    "fr_counter = Counter()\n",
    "\n",
    "_df = df\n",
    "df = []\n",
    "\n",
    "for e, f in _df:\n",
    "\n",
    "    et = en_tokenizer(e.lower())\n",
    "    ft = fr_tokenizer(f.lower())\n",
    "\n",
    "    en_counter.update(et)\n",
    "    fr_counter.update(ft)\n",
    "\n",
    "    df.append([et, ft])\n",
    "\n",
    "print(len(en_counter), len(fr_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab = torchtext.vocab.vocab(en_counter, specials=['<unk>', '<pad>', '<eos>'], special_first=True)\n",
    "fr_vocab = torchtext.vocab.vocab(fr_counter, specials=['<unk>', '<pad>', '<eos>', '<sos>'], special_first=True)\n",
    "\n",
    "PAD_EN = en_vocab['<pad>']\n",
    "PAD_FR = fr_vocab['<pad>']\n",
    "EOS_EN = en_vocab['<eos>']\n",
    "EOS_FR = fr_vocab['<eos>']\n",
    "SOS_FR = fr_vocab['<sos>']\n",
    "\n",
    "en_vocab.set_default_index = en_vocab['<unk>']\n",
    "fr_vocab.set_default_index = fr_vocab['<unk>']\n",
    "\n",
    "del en_counter\n",
    "del fr_counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(batch):\n",
    "    eb = []\n",
    "    fb = [] \n",
    "    for e, f in batch:\n",
    "        tte = torch.LongTensor([en_vocab[k] for k in e])\n",
    "        ttf = torch.LongTensor([fr_vocab[k] for k in f])\n",
    "\n",
    "        eb.append(torch.cat([tte, torch.LongTensor([EOS_EN])], dim=0))\n",
    "        fb.append(torch.cat([ttf, torch.LongTensor([EOS_FR])], dim=0))\n",
    "\n",
    "        en_batch = pad_sequence(eb, batch_first=True, padding_value=PAD_EN)\n",
    "        fr_batch = pad_sequence(fb, batch_first=True, padding_value=PAD_FR)\n",
    "\n",
    "    return en_batch, fr_batch \n",
    "\n",
    "dl = DataLoader(df, batch_size=128, shuffle=True, collate_fn=make_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 18]) torch.Size([128, 22])\n",
      "tensor([129, 258, 890, 121, 460, 278, 129,   9,   2,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1])\n"
     ]
    }
   ],
   "source": [
    "_, batch = next(enumerate(dl))\n",
    "print(batch[0].shape, batch[1].shape)\n",
    "print(batch[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 18, 64])\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module): # TODO: make this bidirectional\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "\n",
    "        # just 1 lstm layer\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.linear = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.embedding(input)\n",
    "        output, _ = self.lstm(embedded)\n",
    "        return self.relu(self.linear(output))\n",
    "\n",
    "# test\n",
    "enc = Encoder(len(en_vocab), 64)\n",
    "o_encoder = enc.forward(batch[0])\n",
    "\n",
    "print(o_encoder.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 23, 64])\n",
      "torch.Size([128, 1, 64])\n",
      "torch.Size([128, 2, 64])\n",
      "torch.Size([128, 3, 64])\n"
     ]
    }
   ],
   "source": [
    "class PreAttentionDecoder(nn.Module):\n",
    "    ### TODO: implement teacher forcing, add to each sequence a <SOS> character\n",
    "    \n",
    "    def __init__(self, target_vocab, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(target_vocab, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True, bidirectional=False)\n",
    "\n",
    "    def forward(self, input, word_by_word_idx=-1):\n",
    "\n",
    "        # shift right with the SOS token\n",
    "        shift_right = torch.LongTensor([SOS_FR] * input.shape[0]).reshape((input.shape[0], 1)).to(device)\n",
    "        input = torch.cat([shift_right, input], dim=1)\n",
    "\n",
    "        # full sentence\n",
    "        if word_by_word_idx == -1: \n",
    "            embedded = self.embedding(input)\n",
    "            output, _ = self.lstm(embedded)\n",
    "            return output\n",
    "\n",
    "        else:\n",
    "            # a single word\n",
    "            embedded = self.embedding(input[:, word_by_word_idx : word_by_word_idx + 1])\n",
    "            if word_by_word_idx == 0:\n",
    "                self.output, self.prev_state = self.lstm(embedded)\n",
    "            else:\n",
    "                output, self.prev_state = self.lstm(embedded, self.prev_state)    \n",
    "                self.output = torch.cat([self.output, output], axis=1)\n",
    "            \n",
    "            return self.output\n",
    "\n",
    "pad = PreAttentionDecoder(len(fr_vocab), 64).to(device)\n",
    "o_preattn = pad.forward(batch[1].to(device))\n",
    "\n",
    "print(o_preattn.shape)\n",
    "\n",
    "o_preattn_word = None\n",
    "for i in range(0, 3):\n",
    "    o_preattn_word = pad.forward(batch[1].to(device), i)\n",
    "    print(o_preattn_word.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, encoder_output, pre_attention_decoder_output):\n",
    "        key = encoder_output # what is input\n",
    "        query = pre_attention_decoder_output # what has been so far translated\n",
    "        value = encoder_output # the value we want to translate\n",
    "\n",
    "        dk = torch.sqrt(torch.Tensor([key.shape[-1]])).to(device)\n",
    "        key_transpose = torch.transpose(key, 1, 2)\n",
    "        \n",
    "        mm = torch.bmm(query, key_transpose) / dk\n",
    "        mm = nn.functional.softmax(mm, dim=-1)\n",
    "        ret = torch.bmm(mm, value)    \n",
    "        return  ret\n",
    "    \n",
    "\n",
    "dpa = ScaledDotProductAttention().to(device)\n",
    "\n",
    "# all at once\n",
    "o_attn = dpa.forward(o_encoder.to(device), o_preattn.to(device))\n",
    "#print(o_attn[0].shape)\n",
    "\n",
    "# word by word\n",
    "o_attn_word = dpa.forward(o_encoder.to(device), o_preattn_word.to(device))\n",
    "#print(o_attn_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2645, 0.2645, 0.4711],\n",
       "         [0.4711, 0.2645, 0.2645],\n",
       "         [0.2645, 0.4711, 0.2645]]], device='mps:0')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# let's check the ScaledDotProductAttention\n",
    "t1 = torch.Tensor([[[0, 0, 1], [1, 0, 0], [0, 1, 0]]]).to(device)\n",
    "t2 = torch.Tensor([[[0, 0, 1], [1, 0, 0], [0, 1, 0]]]).to(device)\n",
    "\n",
    "dpa.forward(t1, t2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 27139])\n",
      "tensor(4677, device='mps:0') tensor(-8.0331, device='mps:0', grad_fn=<MaxBackward1>) tensor([-10.0987,  -9.5480, -11.2692, -10.8985,  -9.9374,  -9.4283, -10.2868,\n",
      "        -10.6778, -10.5338, -10.1518], device='mps:0',\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        NUM_LAYERS = 2\n",
    "\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=NUM_LAYERS)\n",
    "    \n",
    "        self.seq = nn.Sequential(\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size * 2),\n",
    "            nn.BatchNorm1d(hidden_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size * 2, len(fr_vocab)), \n",
    "            nn.LogSoftmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward (self, input, word_idx=-1):\n",
    "\n",
    "        if word_idx == -1 or word_idx == 0:\n",
    "            o, self.h = self.gru(input)\n",
    "        else:\n",
    "            o, self.h = self.gru(input[:, word_idx : word_idx + 1, :], self.h)\n",
    "\n",
    "        # take only the last value\n",
    "        o = o[:, -1, :] \n",
    "        return self.seq(o)\n",
    "    \n",
    "dec = Decoder(64).to(device)\n",
    "o = dec.forward(o_attn)\n",
    "print(o.shape)\n",
    "print(torch.argmax(o[0]), torch.max(o[0]), o[0][0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training now, will get input and predict the last word. boom\n",
    "class Translator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(len(en_vocab), 64)\n",
    "        self.pre_attn_decoder = PreAttentionDecoder(len(fr_vocab), 64)\n",
    "        self.attention = ScaledDotProductAttention()\n",
    "        self.decoder = Decoder(64)\n",
    "        self.past_enc = None\n",
    "\n",
    "    def forward(self, sentence, translated_sequence_so_far, word_by_word_idx = -1):\n",
    "\n",
    "        # either batch prediction or char by char, first time\n",
    "        if word_by_word_idx == -1 or word_by_word_idx == 0:\n",
    "            enc = self.encoder.forward(sentence)\n",
    "            self.past_enc = enc\n",
    "        else:\n",
    "            enc=self.past_enc\n",
    "            \n",
    "        pre_attn = self.pre_attn_decoder.forward(translated_sequence_so_far, word_by_word_idx)\n",
    "        attn = self.attention.forward(enc, pre_attn)\n",
    "        return self.decoder.forward(attn, word_by_word_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translator(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(15962, 64)\n",
      "    (lstm): LSTM(64, 64, batch_first=True, bidirectional=True)\n",
      "    (linear): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (pre_attn_decoder): PreAttentionDecoder(\n",
      "    (embedding): Embedding(27139, 64)\n",
      "    (lstm): LSTM(64, 64, batch_first=True)\n",
      "  )\n",
      "  (attention): ScaledDotProductAttention()\n",
      "  (decoder): Decoder(\n",
      "    (gru): GRU(64, 64, num_layers=2, batch_first=True)\n",
      "    (seq): Sequential(\n",
      "      (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Linear(in_features=64, out_features=128, bias=True)\n",
      "      (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=128, out_features=27139, bias=True)\n",
      "      (5): LogSoftmax(dim=-1)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Total params:  6426115\n",
      "loss: 10.138462, seq_len:   18 [    0/ 1703]\n",
      "loss: 10.092522, seq_len:   24 [   10/ 1703]\n",
      "loss: 10.078456, seq_len:   22 [   20/ 1703]\n",
      "loss: 10.099743, seq_len:   16 [   30/ 1703]\n",
      "loss: 10.019287, seq_len:   31 [   40/ 1703]\n",
      "loss: 10.000386, seq_len:   29 [   50/ 1703]\n",
      "loss: 10.001542, seq_len:   22 [   60/ 1703]\n",
      "loss: 9.984631, seq_len:   20 [   70/ 1703]\n",
      "loss: 10.010354, seq_len:   17 [   80/ 1703]\n",
      "loss: 9.911392, seq_len:   30 [   90/ 1703]\n",
      "loss: 9.946107, seq_len:   21 [  100/ 1703]\n",
      "loss: 9.953515, seq_len:   19 [  110/ 1703]\n",
      "loss: 9.919123, seq_len:   23 [  120/ 1703]\n",
      "loss: 9.892875, seq_len:   22 [  130/ 1703]\n",
      "loss: 9.890012, seq_len:   20 [  140/ 1703]\n",
      "loss: 9.817995, seq_len:   31 [  150/ 1703]\n",
      "loss: 9.834385, seq_len:   25 [  160/ 1703]\n",
      "loss: 9.794520, seq_len:   29 [  170/ 1703]\n",
      "loss: 9.804700, seq_len:   22 [  180/ 1703]\n",
      "loss: 9.746341, seq_len:   31 [  190/ 1703]\n",
      "loss: 9.777219, seq_len:   24 [  200/ 1703]\n",
      "loss: 9.750964, seq_len:   25 [  210/ 1703]\n",
      "loss: 9.740732, seq_len:   24 [  220/ 1703]\n",
      "loss: 9.731159, seq_len:   24 [  230/ 1703]\n",
      "loss: 9.743340, seq_len:   22 [  240/ 1703]\n",
      "loss: 9.735947, seq_len:   21 [  250/ 1703]\n",
      "loss: 9.731190, seq_len:   20 [  260/ 1703]\n",
      "loss: 9.667349, seq_len:   25 [  270/ 1703]\n",
      "loss: 9.668362, seq_len:   24 [  280/ 1703]\n",
      "loss: 9.624492, seq_len:   29 [  290/ 1703]\n",
      "loss: 9.685109, seq_len:   21 [  300/ 1703]\n",
      "loss: 9.632497, seq_len:   24 [  310/ 1703]\n",
      "loss: 9.657830, seq_len:   20 [  320/ 1703]\n",
      "loss: 9.613491, seq_len:   23 [  330/ 1703]\n",
      "loss: 9.609441, seq_len:   21 [  340/ 1703]\n",
      "loss: 9.684973, seq_len:   16 [  350/ 1703]\n",
      "loss: 9.643636, seq_len:   18 [  360/ 1703]\n",
      "loss: 9.584243, seq_len:   20 [  370/ 1703]\n",
      "loss: 9.542413, seq_len:   23 [  380/ 1703]\n",
      "loss: 9.620520, seq_len:   17 [  390/ 1703]\n",
      "loss: 9.585184, seq_len:   18 [  400/ 1703]\n",
      "loss: 9.525225, seq_len:   21 [  410/ 1703]\n",
      "loss: 9.501613, seq_len:   22 [  420/ 1703]\n",
      "loss: 9.521753, seq_len:   20 [  430/ 1703]\n",
      "loss: 9.503900, seq_len:   21 [  440/ 1703]\n",
      "loss: 9.497660, seq_len:   19 [  450/ 1703]\n",
      "loss: 9.435313, seq_len:   23 [  460/ 1703]\n",
      "loss: 9.406523, seq_len:   25 [  470/ 1703]\n",
      "loss: 9.391450, seq_len:   25 [  480/ 1703]\n",
      "loss: 9.422049, seq_len:   23 [  490/ 1703]\n",
      "loss: 9.322855, seq_len:   33 [  500/ 1703]\n",
      "loss: 9.472210, seq_len:   18 [  510/ 1703]\n",
      "loss: 9.359252, seq_len:   25 [  520/ 1703]\n",
      "loss: 9.377376, seq_len:   22 [  530/ 1703]\n",
      "loss: 9.356799, seq_len:   24 [  540/ 1703]\n",
      "loss: 9.235977, seq_len:   47 [  550/ 1703]\n",
      "loss: 9.313784, seq_len:   24 [  560/ 1703]\n",
      "loss: 9.350252, seq_len:   21 [  570/ 1703]\n",
      "loss: 9.302896, seq_len:   24 [  580/ 1703]\n",
      "loss: 9.353989, seq_len:   19 [  590/ 1703]\n",
      "loss: 9.228710, seq_len:   27 [  600/ 1703]\n",
      "loss: 9.254540, seq_len:   25 [  610/ 1703]\n",
      "loss: 9.365566, seq_len:   16 [  620/ 1703]\n",
      "loss: 9.223285, seq_len:   27 [  630/ 1703]\n",
      "loss: 9.299537, seq_len:   18 [  640/ 1703]\n",
      "loss: 9.286693, seq_len:   19 [  650/ 1703]\n",
      "loss: 9.298994, seq_len:   20 [  660/ 1703]\n",
      "loss: 9.199159, seq_len:   25 [  670/ 1703]\n",
      "loss: 9.218738, seq_len:   21 [  680/ 1703]\n",
      "loss: 9.169847, seq_len:   23 [  690/ 1703]\n",
      "loss: 9.264982, seq_len:   17 [  700/ 1703]\n",
      "loss: 9.226374, seq_len:   20 [  710/ 1703]\n",
      "loss: 9.116507, seq_len:   23 [  720/ 1703]\n",
      "loss: 9.141799, seq_len:   23 [  730/ 1703]\n",
      "loss: 9.082033, seq_len:   27 [  740/ 1703]\n",
      "loss: 9.231211, seq_len:   17 [  750/ 1703]\n",
      "loss: 9.142161, seq_len:   21 [  760/ 1703]\n",
      "loss: 9.057675, seq_len:   26 [  770/ 1703]\n",
      "loss: 9.148312, seq_len:   19 [  780/ 1703]\n",
      "loss: 9.160695, seq_len:   19 [  790/ 1703]\n",
      "loss: 9.107964, seq_len:   21 [  800/ 1703]\n",
      "loss: 9.100993, seq_len:   21 [  810/ 1703]\n",
      "loss: 9.102727, seq_len:   20 [  820/ 1703]\n",
      "loss: 8.963291, seq_len:   28 [  830/ 1703]\n",
      "loss: 8.904155, seq_len:   36 [  840/ 1703]\n",
      "loss: 9.028044, seq_len:   23 [  850/ 1703]\n",
      "loss: 9.035408, seq_len:   22 [  860/ 1703]\n",
      "loss: 9.099009, seq_len:   19 [  870/ 1703]\n",
      "loss: 9.024971, seq_len:   21 [  880/ 1703]\n",
      "loss: 9.081426, seq_len:   18 [  890/ 1703]\n",
      "loss: 8.919440, seq_len:   27 [  900/ 1703]\n",
      "loss: 8.883173, seq_len:   29 [  910/ 1703]\n",
      "loss: 8.999316, seq_len:   20 [  920/ 1703]\n",
      "loss: 9.008062, seq_len:   21 [  930/ 1703]\n",
      "loss: 8.907150, seq_len:   25 [  940/ 1703]\n",
      "loss: 9.001084, seq_len:   18 [  950/ 1703]\n",
      "loss: 8.835665, seq_len:   32 [  960/ 1703]\n",
      "loss: 8.844763, seq_len:   28 [  970/ 1703]\n",
      "loss: 8.876238, seq_len:   23 [  980/ 1703]\n",
      "loss: 9.001920, seq_len:   17 [  990/ 1703]\n",
      "loss: 8.959817, seq_len:   19 [ 1000/ 1703]\n",
      "loss: 8.853173, seq_len:   25 [ 1010/ 1703]\n",
      "loss: 8.845621, seq_len:   25 [ 1020/ 1703]\n",
      "loss: 8.845202, seq_len:   23 [ 1030/ 1703]\n",
      "loss: 8.927097, seq_len:   18 [ 1040/ 1703]\n",
      "loss: 8.867513, seq_len:   23 [ 1050/ 1703]\n",
      "loss: 8.881552, seq_len:   20 [ 1060/ 1703]\n",
      "loss: 8.792832, seq_len:   25 [ 1070/ 1703]\n",
      "loss: 8.821431, seq_len:   22 [ 1080/ 1703]\n",
      "loss: 8.851838, seq_len:   21 [ 1090/ 1703]\n",
      "loss: 8.772638, seq_len:   24 [ 1100/ 1703]\n",
      "loss: 8.835481, seq_len:   20 [ 1110/ 1703]\n",
      "loss: 8.700086, seq_len:   27 [ 1120/ 1703]\n",
      "loss: 8.893714, seq_len:   16 [ 1130/ 1703]\n",
      "loss: 8.811481, seq_len:   20 [ 1140/ 1703]\n",
      "loss: 8.703544, seq_len:   27 [ 1150/ 1703]\n",
      "loss: 8.808914, seq_len:   19 [ 1160/ 1703]\n",
      "loss: 8.692185, seq_len:   25 [ 1170/ 1703]\n",
      "loss: 8.623151, seq_len:   33 [ 1180/ 1703]\n",
      "loss: 8.737073, seq_len:   21 [ 1190/ 1703]\n",
      "loss: 8.755090, seq_len:   20 [ 1200/ 1703]\n",
      "loss: 8.622849, seq_len:   27 [ 1210/ 1703]\n",
      "loss: 8.691426, seq_len:   24 [ 1220/ 1703]\n",
      "loss: 8.594189, seq_len:   29 [ 1230/ 1703]\n",
      "loss: 8.679851, seq_len:   22 [ 1240/ 1703]\n",
      "loss: 8.707156, seq_len:   22 [ 1250/ 1703]\n",
      "loss: 8.599007, seq_len:   26 [ 1260/ 1703]\n",
      "loss: 8.581794, seq_len:   26 [ 1270/ 1703]\n",
      "loss: 8.481662, seq_len:   37 [ 1280/ 1703]\n",
      "loss: 8.553013, seq_len:   26 [ 1290/ 1703]\n",
      "loss: 8.461593, seq_len:   36 [ 1300/ 1703]\n",
      "loss: 8.611901, seq_len:   23 [ 1310/ 1703]\n",
      "loss: 8.531100, seq_len:   27 [ 1320/ 1703]\n",
      "loss: 8.654738, seq_len:   19 [ 1330/ 1703]\n",
      "loss: 8.521658, seq_len:   26 [ 1340/ 1703]\n",
      "loss: 8.579559, seq_len:   23 [ 1350/ 1703]\n",
      "loss: 8.597121, seq_len:   22 [ 1360/ 1703]\n",
      "loss: 8.438836, seq_len:   30 [ 1370/ 1703]\n",
      "loss: 8.402744, seq_len:   38 [ 1380/ 1703]\n",
      "loss: 8.645067, seq_len:   19 [ 1390/ 1703]\n",
      "loss: 8.531661, seq_len:   23 [ 1400/ 1703]\n",
      "loss: 8.641736, seq_len:   19 [ 1410/ 1703]\n",
      "loss: 8.576960, seq_len:   21 [ 1420/ 1703]\n",
      "loss: 8.361123, seq_len:   36 [ 1430/ 1703]\n",
      "loss: 8.458477, seq_len:   25 [ 1440/ 1703]\n",
      "loss: 8.564972, seq_len:   20 [ 1450/ 1703]\n",
      "loss: 8.453000, seq_len:   23 [ 1460/ 1703]\n",
      "loss: 8.462079, seq_len:   23 [ 1470/ 1703]\n",
      "loss: 8.506239, seq_len:   21 [ 1480/ 1703]\n",
      "loss: 8.323213, seq_len:   32 [ 1490/ 1703]\n",
      "loss: 8.556418, seq_len:   18 [ 1500/ 1703]\n",
      "loss: 8.492026, seq_len:   20 [ 1510/ 1703]\n",
      "loss: 8.502735, seq_len:   19 [ 1520/ 1703]\n",
      "loss: 8.442863, seq_len:   22 [ 1530/ 1703]\n",
      "loss: 8.474283, seq_len:   20 [ 1540/ 1703]\n",
      "loss: 8.381864, seq_len:   24 [ 1550/ 1703]\n",
      "loss: 8.353070, seq_len:   26 [ 1560/ 1703]\n",
      "loss: 8.191055, seq_len:   46 [ 1570/ 1703]\n",
      "loss: 8.251745, seq_len:   31 [ 1580/ 1703]\n",
      "loss: 8.404402, seq_len:   21 [ 1590/ 1703]\n",
      "loss: 8.367267, seq_len:   23 [ 1600/ 1703]\n",
      "loss: 8.385574, seq_len:   22 [ 1610/ 1703]\n",
      "loss: 8.411420, seq_len:   19 [ 1620/ 1703]\n",
      "loss: 8.366829, seq_len:   21 [ 1630/ 1703]\n",
      "loss: 8.280990, seq_len:   23 [ 1640/ 1703]\n",
      "loss: 8.275760, seq_len:   25 [ 1650/ 1703]\n",
      "loss: 8.210520, seq_len:   27 [ 1660/ 1703]\n",
      "loss: 8.353377, seq_len:   19 [ 1670/ 1703]\n",
      "loss: 8.391309, seq_len:   20 [ 1680/ 1703]\n",
      "loss: 8.260784, seq_len:   23 [ 1690/ 1703]\n",
      "loss: 8.198476, seq_len:   28 [ 1700/ 1703]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tn = torch.load(\"translation_model.mdl\")\n",
    "except:\n",
    "    tn = Translator().to(device)\n",
    "\n",
    "print(tn)\n",
    "print(\"Total params: \", sum(p.numel() for p in tn.parameters() if p.requires_grad))\n",
    "\n",
    "# training\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(tn.parameters(), lr=1e-5) \n",
    "\n",
    "def train(model, dataloader):\n",
    "    size = len(dataloader)\n",
    "    model.train()\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        loss = 0\n",
    "\n",
    "        for i in range(0, y.shape[1]):\n",
    "\n",
    "            translated_sentence = y[:, 0:i]\n",
    "            result = y[:, i]\n",
    "\n",
    "            # Compute prediction error\n",
    "            pred = model(X, translated_sentence, i)\n",
    "            loss += loss_fn(pred, result)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(tn.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), (batch)\n",
    "            #max_param =torch.max(torch.nn.utils.parameters_to_vector(tn.parameters())).item()\n",
    "            seq_len = y.shape[1]\n",
    "\n",
    "            loss /= y.shape[1] # normalize with the length of the batch\n",
    "            print(f\"loss: {loss:>7f}, seq_len: {seq_len:>4d} [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "train(tn, dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(tn, \"translation_model.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11])\n",
      "['être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être', 'être']\n"
     ]
    }
   ],
   "source": [
    "def translate(str_en):\n",
    "\n",
    "    cnt = 2 * len(str_en)\n",
    "    str_en = en_tokenizer(str_en.lower())\n",
    "    \n",
    "    tn.train(False)\n",
    "\n",
    "    # en\n",
    "    en = torch.LongTensor([en_vocab[k] for k in str_en] + [EOS_EN]).to(device)\n",
    "    # batch dimension\n",
    "    en = en[None, :]\n",
    "    print(en.shape)\n",
    "    ret = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = -1\n",
    "\n",
    "        while pred != EOS_FR and cnt > 0:\n",
    "            so_far = torch.LongTensor(ret)[None, :].to(device)\n",
    "            pred = tn(en, so_far)\n",
    "            pred = torch.argmax(pred)\n",
    "            ret.append(pred.item())\n",
    "            cnt -= 1\n",
    "\n",
    "    print(fr_vocab.lookup_tokens(ret))\n",
    "\n",
    "\n",
    "translate(\"Let's go! Let's move from here!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translator(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(15962, 64)\n",
      "    (lstm): LSTM(64, 64, batch_first=True)\n",
      "  )\n",
      "  (pre_attn_decoder): PreAttentionDecoder(\n",
      "    (embedding): Embedding(27139, 64)\n",
      "    (lstm): LSTM(64, 64, batch_first=True)\n",
      "  )\n",
      "  (attention): ScaledDotProductAttention()\n",
      "  (decoder): Decoder(\n",
      "    (gru): GRU(64, 64, num_layers=2, batch_first=True)\n",
      "    (seq): Sequential(\n",
      "      (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=128, out_features=27139, bias=True)\n",
      "      (5): LogSoftmax(dim=-1)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "-4.939054012298584 4.701839447021484\n",
      "inf -inf\n",
      "inf -inf\n",
      "inf -inf\n",
      "inf -inf\n",
      "-4.940924167633057 4.736354827880859\n",
      "inf -inf\n",
      "inf -inf\n",
      "inf -inf\n",
      "inf -inf\n",
      "inf -inf\n",
      "inf -inf\n",
      "inf -inf\n",
      "inf -inf\n",
      "inf -inf\n",
      "inf -inf\n",
      "inf -inf\n",
      "inf -inf\n",
      "inf -inf\n",
      "inf -inf\n",
      "inf -inf\n",
      "inf -inf\n",
      "inf -inf\n",
      "-0.017060045152902603 0.022138481959700584\n",
      "-0.10901685804128647 0.10518678277730942\n",
      "-0.1200936958193779 0.11053194105625153\n"
     ]
    }
   ],
   "source": [
    "def min_max_params():\n",
    "    for p in tn.parameters():\n",
    "    \n",
    "        max__ = p.max().item()\n",
    "        min__ = p.min().item()\n",
    "\n",
    "        print(min__, max__)\n",
    "\n",
    "min_max_params()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
